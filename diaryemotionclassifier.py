# -*- coding: utf-8 -*-
"""DiaryEmotionClassifier.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WdG9-qsSNgrz72ozGBSB0-IjqdeU6wfG

# Diary Emotion Classifier

# Load in raw dataset
"""

import pandas as pd

df = pd.read_csv("Raw.csv")

df

"""# Word segmentation as a requirement of PhoBert model

"""

script = df["Script"]
script

#Import the segmentation model from HuggingFace

from transformers import AutoTokenizer, AutoModelForTokenClassification
from transformers import pipeline

tokenizer = AutoTokenizer.from_pretrained("NlpHUST/vi-word-segmentation")
model = AutoModelForTokenClassification.from_pretrained("NlpHUST/vi-word-segmentation")

nlp = pipeline("token-classification", model=model, tokenizer=tokenizer)

segmented_script = []

for sentence in script:

  ner_results = nlp(sentence)
  example_tok = ""
  for e in ner_results:
      if "##" in e["word"]:
          example_tok = example_tok + e["word"].replace("##","")
      elif e["entity"] =="I":
          example_tok = example_tok + "_" + e["word"]
      else:
          example_tok = example_tok + " " + e["word"]

  segmented_script.append(example_tok)

#Check the len and type
len(segmented_script), type(segmented_script)

df["Segmented_script"] = segmented_script
df

"""# TOKENIZATION BY PHOBERT (VinAI)"""

segmented = df["Segmented_script"]
segmented

import torch
from transformers import AutoModel, AutoTokenizer
import numpy as IhateNumpy

phobert = AutoModel.from_pretrained("vinai/phobert-base-v2")
tokenizer = AutoTokenizer.from_pretrained("vinai/phobert-base-v2")


tokenized = []
i = 0

for script in segmented:
  print(f"Number of sentence being tokenized: {i}")
  input_ids = IhateNumpy.array(([tokenizer.encode(script)]))
  tokenized.append(input_ids)
  i+=1

len(tokenized), type(tokenized[0])

df["Tokenized"] = tokenized

"""# Padding to ensure balance data for training"""

tokenized = df["Tokenized"]
padded = []

max_len_to_padding = 700

i =0

for sentence in tokenized:
  diff = max_len_to_padding - sentence.shape[1]
  padding_tensor = IhateNumpy.zeros((diff))

  padded_sentence = IhateNumpy.hstack((sentence.squeeze(), padding_tensor))

  padded.append(padded_sentence)

normal_padded = []

for sample in padded:
  new_sample = sample.tolist()
  normal_padded.append(new_sample)

df["Padded"] = padded

df

"""# Change emotion label to numerical label for training"""

import numpy as np

happy = np.full((50), 0)
sad = np.full((50),1)
angry = np.full((50),2)
suprise = np.full((50),3)
bored = np.full((50),4)


numerical_label = np.concatenate((happy, sad, angry, suprise, bored))

"""# Split dataset"""

from sklearn.model_selection import train_test_split

# Extract the 'Padded' column as a NumPy array of objects
training_feature = np.array(df['Padded'].tolist())

X, Y = torch.from_numpy(training_feature).type(torch.float32), torch.from_numpy(numerical_label).type(torch.LongTensor)
X.shape

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2)

len(X_train), len(X_test), len(Y_train), len(Y_test)

"""# Construct a model"""

from torch import nn

import torch
import torch.nn as nn

class EmotionClassification(nn.Module):
    def __init__(self):
        super().__init__()
        self.linear_stack = nn.Sequential(
            nn.Linear(in_features=700, out_features=1024),
            nn.ReLU(),
            nn.BatchNorm1d(1024),
            nn.Dropout(0.5),
            nn.Linear(in_features=1024, out_features=2048),
            nn.ReLU(),
            nn.BatchNorm1d(2048),
            nn.Dropout(0.5),
            nn.Linear(in_features=2048, out_features=2048),
            nn.ReLU(),
            nn.BatchNorm1d(2048),
            nn.Dropout(0.5),
            nn.Linear(in_features=2048, out_features=1024),
            nn.ReLU(),
            nn.Linear(in_features=1024, out_features=5),
        )

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return self.linear_stack(x)

model = EmotionClassification()

"""# Loss function and Optimizer"""

loss_function = nn.CrossEntropyLoss() #Compare logits(raw) output of model WITH REAL LABEL
optimizer = torch.optim.Adam(params = model.parameters(), lr = 0.05)

#Calculate accuracy

def accuracy_fn(Y_true, Y_pred):
  correct = torch.eq(Y_true, Y_pred) #return HOW MANY CORRECT PREDICTION
  acc = (correct/len(Y_pred))*100

  return acc

"""# Training loop"""

#TRAINING LOOP
epochs = 1000

for epoch in range(epochs):
  model.train()

  train_logits = model(X_train)
  train_pred = torch.argmax(torch.softmax(train_logits, dim = 1), dim = 1)

  train_loss = loss_function(train_logits, Y_train)
  train_acc = accuracy_fn(Y_train, train_pred)

  optimizer.zero_grad()

  train_loss.backward()

  optimizer.step()

  #Evaluation
  model.eval()

  with torch.inference_mode():
    test_logits = model(X_test)
    test_pred = torch.argmax(torch.softmax(test_logits, dim =1), dim =1)

    test_loss = loss_function(test_logits, Y_test)
    test_acc = accuracy_fn(Y_test, test_pred)

  #Visualize
  if epoch%100 == 0:
    print(f"Epoch: {epoch} | Train loss : {train_loss} | Test loss : {test_loss}")

"""# Model evaluation"""

